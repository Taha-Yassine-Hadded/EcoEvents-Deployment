

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import pandas as pd
import joblib
import logging
from typing import Dict
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import os

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# T√©l√©chargement des ressources NLTK
nltk.download('punkt')
nltk.download('stopwords')

app = FastAPI()

# Liste des cat√©gories valides
VALID_CATEGORIES = [
    'recyclage', 'climat', 'biodiversite', 'eau', 'energie',
    'transport', 'alimentation', 'pollution', 'sensibilisation'
]

# Pr√©traitement du texte
stemmer = SnowballStemmer('french')
stop_words = set(stopwords.words('french'))

def preprocess_text(text: str) -> str:
    tokens = word_tokenize(text.lower())
    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]
    return ' '.join(tokens)

# Cr√©er le dossier models/ s'il n'existe pas
models_dir = 'models'
if not os.path.exists(models_dir):
    os.makedirs(models_dir)
    logger.info(f"‚úÖ Dossier {models_dir} cr√©√© avec succ√®s")

# Charger et filtrer les donn√©es
try:
    df = pd.read_csv('data/category.csv', sep=';')
    # Filtrer les cat√©gories valides
    df = df[df['category'].isin(VALID_CATEGORIES)]
    if df.empty:
        raise ValueError("Aucune cat√©gorie valide trouv√©e dans category.csv")
    logger.info(f"‚úÖ Donn√©es charg√©es avec succ√®s : {len(df)} cat√©gories valides")
except Exception as e:
    logger.error(f"Erreur lors du chargement des donn√©es : {str(e)}")
    raise Exception(f"Erreur lors du chargement des donn√©es : {str(e)}")

# Pr√©parer les donn√©es pour l'entra√Ænement
X = df['keywords'].apply(preprocess_text)
y = df['category']

# V√©rifier si les donn√©es sont suffisantes
if len(X) < 1:
    logger.error("Donn√©es insuffisantes pour l'entra√Ænement : minimum 1 √©chantillon requis")
    raise ValueError("Donn√©es insuffisantes pour l'entra√Ænement")

# Vectorisation et entra√Ænement du mod√®le
vectorizer = TfidfVectorizer(max_features=1000)
X_vectorized = vectorizer.fit_transform(X)

model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)
model.fit(X_vectorized, y)
logger.info("‚úÖ Mod√®le entra√Æn√© avec succ√®s")

# Sauvegarder le mod√®le et le vectoriseur
try:
    joblib.dump(vectorizer, os.path.join(models_dir, 'tfidf_vectorizer.pkl'))
    joblib.dump(model, os.path.join(models_dir, 'classification_model.pkl'))
    logger.info("‚úÖ Mod√®le et vectoriseur sauvegard√©s avec succ√®s")
except Exception as e:
    logger.error(f"Erreur lors de la sauvegarde du mod√®le : {str(e)}")
    raise Exception(f"Erreur lors de la sauvegarde du mod√®le : {str(e)}")

class TextInput(BaseModel):
    text: str

@app.post("/predict-category")
async def predict_category(input: TextInput) -> Dict:
    try:
        logger.info(f"üì• Texte re√ßu : {input.text[:50]}...")
        processed_text = preprocess_text(input.text)
        if len(processed_text.split()) < 10:
            raise HTTPException(status_code=400, detail="Texte trop court pour une pr√©diction fiable")
        text_vector = vectorizer.transform([processed_text])
        probabilities = model.predict_proba(text_vector)[0]
        predicted_category = model.classes_[probabilities.argmax()]

        # V√©rifier si la cat√©gorie pr√©dite est valide
        if predicted_category not in VALID_CATEGORIES:
            logger.warning(f"Cat√©gorie pr√©dite non valide : {predicted_category}. Retour √† la cat√©gorie la plus probable parmi les valides.")
            valid_indices = [i for i, cat in enumerate(model.classes_) if cat in VALID_CATEGORIES]
            if not valid_indices:
                raise HTTPException(status_code=500, detail="Aucune cat√©gorie valide trouv√©e dans le mod√®le")
            valid_probabilities = probabilities[valid_indices]
            valid_category_index = valid_indices[valid_probabilities.argmax()]
            predicted_category = model.classes_[valid_category_index]

        confidence = float(probabilities.max())
        probabilities_dict = {model.classes_[i]: float(prob) for i, prob in enumerate(probabilities)}

        response = {
            "success": True,
            "data": {
                "category": predicted_category,
                "confidence": confidence,
                "all_probabilities": probabilities_dict
            }
        }
        return response
    except Exception as e:
        logger.error(f"Erreur lors de la pr√©diction : {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))






if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=6000, reload=True)
